# Toxic comment classification project

## Problem statement

The goal of this project is to build a classification model that allows to detect different types of toxicity (obscenity, threats, insults, and identity-based hate). 

The initial project and data come from the [Conversation AI team](https://conversationai.github.io/), a research initiative founded by [Jigsaw](https://jigsaw.google.com/) and Google (both a part of Alphabet). They are working on tools to help improve online conversations. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion)

## Data sets

The data is available on [Kaggle competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)

Pretrained GloVe word embeddings can be found [here](https://www.kaggle.com/danielwillgeorge/glove6b100dtxt)

## Project requirements

numpy

pandas

matplotlib.pyplot

seaborn

sklearn

keras

tensorflow

wordcloud

imblearn
